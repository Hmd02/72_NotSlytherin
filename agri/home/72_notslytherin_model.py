# -*- coding: utf-8 -*-
"""72_NOTSLYTHERIN_MODEL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Hmd02/72_NotSlytherin_2/blob/main/72_NOTSLYTHERIN_MODEL.ipynb
"""

from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from matplotlib import pyplot as plt
import seaborn as sb
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings 
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)
from xgboost import XGBRegressor
import tensorflow as tf



def createTrain():
  merged = pd.read_csv('merged.csv')
  merged.drop(['Unnamed: 0','ANNUAL','Jul-Oct', 'Nov-Mar', 'Apr-Jun', 'Total_Avg','Crop_Year'],axis=1,inplace=True)
  merged.drop(['District_Name'],axis=1,inplace=True)
  # print(merged.loc[merged["Crop"]=="Rice",:])
  # print(merged.loc[merged["Crop"]=="Wheat",:])
  # print(merged.loc[merged["Crop"]=="Jowar",:])
  # print(merged.loc[merged["Crop"]=="Bajra",:])
  # print(merged.loc[merged["Crop"]=="Coconut",:])
  # df=merged.copy()
  # df=merged.loc[(merged["Crop"]=="Rice")|(merged["Crop"]=="Wheat")|(merged["Crop"]=="Barley")|(merged["Crop"]=="Jowar"),:]
  # print(df.Production.count())
  train = merged.loc[:146000, :]
  test = merged.iloc[146000:, :]
  train.dropna(subset=['Production'],inplace=True,axis=0)
  # df=merged.loc[(merged["Crop"]=="Rice")|(merged["Crop"]=="Wheat")|(merged["Crop"]=="Barley")|(merged["Crop"]=="Jowar"),:]
  # print(df.count())
  test.drop('Production', axis=1,inplace=True)
  train.to_csv('train.csv')
  test.to_csv('test.csv')
  return merged
createTrain()

def get_data():
    #get train data
    train_data_path ='train.csv'
    train = pd.read_csv(train_data_path)
    
    #get test data
    test_data_path ='test.csv'
    test = pd.read_csv(test_data_path)
    
    return train , test

def get_combined_data():
  #reading train data
  train , test = get_data()
  
  
  target = train.Production
  # print(train.isnull().sum())
  # print(target.isnull().sum())
  train.drop(['Production'],axis = 1 , inplace = True)
  # print(train.count())

  combined = train.append(test)
  combined.reset_index(inplace=True)
  combined.drop(combined.columns[combined.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)
  combined.drop('index', axis=1, inplace = True)
  return combined, target

#Load train and test data into pandas DataFrames
train_data, test_data = get_data()

#Combine train and test data to process them together
combined, target = get_combined_data()

# def createTrain():
#   merged = pd.read_csv('/content/merged.csv')
#   train = merged.loc[:146000, :]
#   test = merged.iloc[146000:, :]
#   test.drop('Production', axis=1,inplace=True)
#   print(test['Total_Avg'])
#   train.to_csv('train.csv')
#   test.to_csv('test.csv')

# createTrain()

def split_combined():
    global combined
    train = combined[:144691]
    test = combined[144691:]

    return train , test



combined.columns

def oneHotEncode(df,colNames):
    for col in colNames:
        if( df[col].dtype == np.dtype('object')):
            dummies = pd.get_dummies(df[col],prefix=col)
            df = pd.concat([df,dummies],axis=1)

            #drop the encoded column
            df.drop([col],axis = 1 , inplace=True)
    return df

print('There were {} columns before encoding categorical features'.format(combined.shape[1]))
combined = oneHotEncode(combined, combined.columns)
print('There are {} columns after encoding categorical features'.format(combined.shape[1]))

def get_cols_with_no_nans(df,col_type):
    '''
    Arguments :
    df : The dataframe to process
    col_type : 
          num : to only get numerical columns with no nans
          no_num : to only get nun-numerical columns with no nans
          all : to get any columns with no nans    
    '''
    if (col_type == 'num'):
        predictors = df.select_dtypes(exclude=['object'])
    elif (col_type == 'no_num'):
        predictors = df.select_dtypes(include=['object'])
    elif (col_type == 'all'):
        predictors = df
    else :
        print('Error : choose a type (num, no_num, all)')
        return 0
    cols_with_no_nans = []
    for col in predictors.columns:
        if not df[col].isnull().any():
            cols_with_no_nans.append(col)
    return cols_with_no_nans

num_cols = get_cols_with_no_nans(combined , 'num')
cat_cols = get_cols_with_no_nans(combined , 'no_num')

print ('Number of numerical columns with no nan values :',len(num_cols))
print ('Number of nun-numerical columns with no nan values :',len(cat_cols))

# print('There were {} columns before encoding categorical features'.format(combined.shape[1]))
# combined = oneHotEncode(combined, combined.columns)
# print('There are {} columns after encoding categorical features'.format(combined.shape[1]))

#train.loc[:146000, 'Total_Avg']

train, test = split_combined()

NN_model = Sequential()

print(train.shape[1])

NN_model.add(Dense(153, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))

NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))

NN_model.add(Dense(1, kernel_initializer='normal',activation='softmax'))

# sgd = tf.keras.optimizers.SGD(learning_rate=0.011)

for col in combined.columns:
  if combined[col].isnull().sum()<0:
    print(col)

NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

target.count()

train.count()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

NN_model.fit(train, target, epochs=10, batch_size=256, validation_split = 0.2, callbacks=callbacks_list)

# from sklearn.preprocessing import MinMaxScaler
# import numpy as np
  
# copy the data
# df_sklearn = createTrain().copy()
  
# # apply normalization techniques
# column = 'Area'
# tar='Production'
# rainfall='Rainfall'
# df_sklearn[column] = MinMaxScaler().fit_transform(np.array(df_sklearn[column]).reshape(-1,1))
# df_sklearn[tar] = MinMaxScaler().fit_transform(np.array(df_sklearn[tar]).reshape(-1,1))
# df_sklearn[rainfall] = MinMaxScaler().fit_transform(np.array(df_sklearn[rainfall]).reshape(-1,1))
  
# view normalized data  
# df_sklearn.corr()

# Load wights file of the best model :
wights_file = 'Weights-001--45858.72266.hdf5' # choose the best checkpoint 
NN_model.load_weights(wights_file) # load it
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])

predictions = NN_model.predict(test)

print(predictions[100][0])